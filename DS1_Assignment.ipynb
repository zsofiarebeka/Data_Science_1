{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a294ca1f-c0f5-44c8-b6c5-db4ee4b59090",
   "metadata": {},
   "source": [
    "# <center> Data Science 1 - Final Assignment <center>\n",
    "<center>Created by Zsófia Rebeka Katona<center>\n",
    "\n",
    "    ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098216e-d606-43a8-bbba-552ab40a3245",
   "metadata": {},
   "source": [
    "The goal of this assignment is to explore the concepts of ridge regression and principal component analysis (PCA) in the context of predictive modeling. We'll examine two exercises:\n",
    "\n",
    "1. Ridge Regression Analysis:\n",
    "\n",
    "- We begin by considering a simple predictive model where the response variable is predicted using only a constant term. We then introduce ridge regression, which is a regularized version of linear regression, and compare its performance with ordinary least squares (OLS) regression.\n",
    "- We generate a sample dataset with a known true parameter and noise distribution. Using this dataset, we compute ridge regression estimates for various values of the regularization parameter lambda (λ) and analyze the bias, variance, and mean squared error (MSE) of these estimates.\n",
    "- Finally, we plot the bias, variance, and MSE as functions of lambda to interpret the results and determine whether ridge regression provides better predictions than OLS regression.\n",
    "        \n",
    "2. Principal Component Analysis (PCA):\n",
    "\n",
    "- We then move on to the second exercise, which involves a dense regression model with 50 correlated predictors.\n",
    "Using PCA, we compute the principal components of the predictors and their corresponding scores.\n",
    "- We estimate OLS regression models using both the original predictors and the principal components on a training sample. Then, we use these models to predict the outcomes in a test sample and compute the mean squared prediction error (MSPE) for comparison.\n",
    "- Finally, we discuss and explain the patterns observed in the MSPE for different sample sizes in comparison with a reference table provided in lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898052d9-729c-4180-be4a-946c56f10173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978c1df-4211-455b-adb9-0879ea8aeb3c",
   "metadata": {},
   "source": [
    "## 1. Ridge Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4551be9-3098-4d53-9189-06a5d4dbc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed\n",
    "np.random.seed(20240315)\n",
    "\n",
    "# Settings for the sample size and repetition\n",
    "n = 500\n",
    "R = 1000 # Repeating the part b) 1000 times - R mentuoned in 1. c)\n",
    "\n",
    "# Defining a true function expecting the feature matrix as input\n",
    "def f(X):\n",
    "    return X[:, 0]**3 - 3.5*X[:, 0]**2 + 3*X[:, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
