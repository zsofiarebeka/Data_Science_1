{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a294ca1f-c0f5-44c8-b6c5-db4ee4b59090",
   "metadata": {},
   "source": [
    "# <center> Data Science 1 - Final Assignment <center>\n",
    "<center>Created by Zsófia Rebeka Katona<center>\n",
    "\n",
    "    ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098216e-d606-43a8-bbba-552ab40a3245",
   "metadata": {},
   "source": [
    "The goal of this assignment is to explore the concepts of ridge regression and principal component analysis (PCA) in the context of predictive modeling. We'll examine two exercises:\n",
    "\n",
    "1. Ridge Regression Analysis:\n",
    "\n",
    "- We begin by considering a simple predictive model where the response variable is predicted using only a constant term. We then introduce ridge regression, which is a regularized version of linear regression, and compare its performance with ordinary least squares (OLS) regression.\n",
    "- We generate a sample dataset with a known true parameter and noise distribution. Using this dataset, we compute ridge regression estimates for various values of the regularization parameter lambda (λ) and analyze the bias, variance, and mean squared error (MSE) of these estimates.\n",
    "- Finally, we plot the bias, variance, and MSE as functions of lambda to interpret the results and determine whether ridge regression provides better predictions than OLS regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898052d9-729c-4180-be4a-946c56f10173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978c1df-4211-455b-adb9-0879ea8aeb3c",
   "metadata": {},
   "source": [
    "## 1. Ridge Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4551be9-3098-4d53-9189-06a5d4dbc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed\n",
    "np.random.seed(20240315)\n",
    "\n",
    "# Settings\n",
    "n = 100\n",
    "sigma = 1\n",
    "beta_zero = 2\n",
    "epsilon = np.random.normal(loc = 0, scale = sigma, size = n)\n",
    "y = beta_zero + epsilon\n",
    "\n",
    "# Creating the OLS fuction\n",
    "beta_ols = np.mean(Y)\n",
    "\n",
    "# Setting the evaluation parameters\n",
    "x_to_evaluate = np.array([[0, 0]])\n",
    "alphas_to_try = np.arange(0.01, 0.5, 0.02)\n",
    "\n",
    "results = np.empty(len(alphas_to_try))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad587947-3166-4bb5-aef3-19b69ecc81c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00634562, 0.13753596, 0.26143482, 0.37223659, 0.47357481,\n",
       "        0.56313742, 0.64057492, 0.70877762, 0.76713541, 0.8162697 ,\n",
       "        0.85749461, 0.88947128, 0.91502421, 0.93528656, 0.95058186,\n",
       "        0.96239506, 0.97139741, 0.97774684, 0.98221055, 0.98551271,\n",
       "        0.98760404, 0.98894507, 0.98987615, 0.99049462, 0.99060436]),\n",
       " array([1.5960561 , 1.30833153, 1.07833484, 0.8904092 , 0.73866206,\n",
       "        0.61573652, 0.5179621 , 0.44139926, 0.38029965, 0.33219423,\n",
       "        0.29597748, 0.26828762, 0.24815833, 0.23350876, 0.22240506,\n",
       "        0.21489912, 0.20967495, 0.20580029, 0.2032034 , 0.20126642,\n",
       "        0.20039972, 0.20003824, 0.19988344, 0.20002622, 0.20002846]),\n",
       " array([1.59609636, 1.32724767, 1.146683  , 1.02896928, 0.96293516,\n",
       "        0.93286027, 0.92829833, 0.94376497, 0.96879638, 0.99849045,\n",
       "        1.03127448, 1.05944678, 1.08542763, 1.10826971, 1.12601093,\n",
       "        1.14110338, 1.15328788, 1.16178917, 1.16794097, 1.17250171,\n",
       "        1.17576147, 1.17805059, 1.17973823, 1.18110581, 1.18132546]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso simulation\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def f(X): \n",
    "    # f(X) = x1 + x2\n",
    "    # : minden sor a nullás oszlopból és minden sor az egyes oszlopból\n",
    "    return X[:, 0] + X[:, 1]\n",
    "\n",
    "\n",
    "n = 20\n",
    "R = 1000\n",
    "# [[]] refers to\n",
    "# We are evaluating at 0, 0\n",
    "x_to_evaluate = np.array([[0, 0]])\n",
    "alphas_to_try = np.arange(0.01, 0.5, 0.02)\n",
    "\n",
    "# Monte Carlo\n",
    "# empty array contains R rows and 1 column\n",
    "results = np.empty((R, len(alphas_to_try)))\n",
    "for _ in range(R):\n",
    "    \n",
    "    # Generate data\n",
    "    X1, X2 = [np.random.uniform(0, 1, n) for _ in range(2)]\n",
    "    X = np.column_stack((X1, X2))\n",
    "    # variance is going to be 2\n",
    "    epsilon = np.random.normal(0, 2, n)\n",
    "    Y = f(X) + epsilon\n",
    "\n",
    "\n",
    "    # Estimate LASSO\n",
    "    for id_a, a in enumerate(alphas_to_try):\n",
    "        lasso = Lasso(alpha = a).fit(X, Y)\n",
    "        pred = lasso.predict(x_to_evaluate)\n",
    "        results [_, id_a] = pred\n",
    "\n",
    "results\n",
    "# Compute bias, variance and mse\n",
    "bias = np.mean(results - f(x_to_evaluate), axis = 0) \n",
    "variance = np.var(results, axis = 0)\n",
    "mse = np.mean(((results - f(x_to_evaluate))**2), axis = 0)\n",
    "bias, variance, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd61124-0741-4886-8ccd-5b1275fa20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1000 # Repeating the part b) 1000 times - R mentuoned in 1. c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796cdb95-4c8c-44ef-8f29-d008f8df5462",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Linear regression\n",
    " Suppose we estimate the regression coefficients in a linear regression\n",
    "model by minimizing the function provided for a particular value of s. For parts (a) through (e), indicate which\n",
    "of i. through v. is correct. Justify your answer.\n",
    "\n",
    "- (a) As we increase s from 0, the training RSS will:\n",
    "    - i. Increase initially, and then eventually start decreasing in an inverted U shape.\n",
    "    - ii. Decrease initially, and then eventually start increasing in a U shape.\n",
    "    - iii. Steadily increase.\n",
    "    - iv. Steadily decrease.\n",
    "    - v. Remain constant.\n",
    "- (b) Repeat (a) for test RSS.\n",
    "- (c) Repeat (a) for variance.\n",
    "- (d) Repeat (a) for (squared) bias.\n",
    "- (e) Repeat (a) for the irreducible error.\n",
    "\n",
    "---\n",
    "\n",
    "Objective:\n",
    "\n",
    "The exercise aims to understand how the training residual sum of squares (RSS), test RSS, variance, squared bias, and irreducible error change as we vary the regularization parameter s in a linear regression model.\n",
    "Linear Regression with Regularization:\n",
    "\n",
    "In a linear regression model, we aim to minimize the difference between observed and predicted values of the response variable (RSS). However, in this exercise, we impose a constraint on the magnitude of the regression coefficients (β) such that the sum of absolute values of coefficients does not exceed a certain threshold s.\n",
    "Effect of Increasing s:\n",
    "\n",
    "As s increases from 0, the constraint becomes less restrictive, allowing larger coefficient values.\n",
    "\n",
    "Let's analyze the expected behavior for each of the metrics:\n",
    "\n",
    "(a) Training RSS: Initially, with a small s, the model is heavily regularized, leading to underfitting and higher training RSS. As s increases, the model becomes less regularized, fitting the training data better, and thus training RSS decreases. However, if s becomes too large, overfitting may occur, causing the training RSS to increase again.\n",
    "\n",
    "(b) Test RSS: The test RSS typically follows a similar pattern to the training RSS. Initially, with a small s, the model generalizes poorly to unseen data, leading to high test RSS. As s increases, the model becomes better at generalizing, resulting in lower test RSS. However, if s becomes too large, overfitting may occur, causing test RSS to increase again.\n",
    "\n",
    "(c) Variance: Variance tends to decrease as s increases because the model becomes less complex and more stable.\n",
    "\n",
    "(d) Squared Bias: Bias tends to decrease initially as s increases because the model becomes more flexible and can capture more complex relationships in the data. However, if s becomes too large, bias may increase due to overfitting.\n",
    "\n",
    "(e) Irreducible Error: Irreducible error remains constant regardless of the value of s because it represents the inherent noise in the data that cannot be reduced by any model.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "In summary, as we increase s from 0:\n",
    "Training RSS and test RSS are expected to initially increase and then decrease or remain relatively stable, depending on the balance between model flexibility and overfitting.\n",
    "Variance is expected to decrease.\n",
    "Bias is expected to decrease initially and then may increase if overfitting occurs.\n",
    "Irreducible error remains constant.\n",
    "This exercise helps in understanding the trade-offs involved in choosing the appropriate level of regularization in a linear regression model and its impact on various aspects of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776fd57-3df3-43fa-8ed0-5ca9dbddd4a1",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA):\n",
    "\n",
    "- We then move on to the second exercise, which involves a dense regression model with 50 correlated predictors.\n",
    "Using PCA, we compute the principal components of the predictors and their corresponding scores.\n",
    "- We estimate OLS regression models using both the original predictors and the principal components on a training sample. Then, we use these models to predict the outcomes in a test sample and compute the mean squared prediction error (MSPE) for comparison.\n",
    "- Finally, we discuss and explain the patterns observed in the MSPE for different sample sizes in comparison with a reference table provided in lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a50ed-c57a-4ce2-afdd-228041203312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
